{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the MNIST example from Liu et al; adapted for output scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dict = {\n",
    "    'AdamW': torch.optim.AdamW,\n",
    "    'Adam': torch.optim.Adam,\n",
    "    'SGD': torch.optim.SGD\n",
    "}\n",
    "\n",
    "activation_dict = {\n",
    "    'ReLU': nn.ReLU,\n",
    "    'Tanh': nn.Tanh,\n",
    "    'Sigmoid': nn.Sigmoid,\n",
    "    'GELU': nn.GELU\n",
    "}\n",
    "\n",
    "loss_function_dict = {\n",
    "    'MSE': nn.MSELoss,\n",
    "    'CrossEntropy': nn.CrossEntropyLoss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_points = 960\n",
    "optimization_steps = 100001\n",
    "batch_size = 16\n",
    "loss_function = 'MSE'   # 'MSE' or 'CrossEntropy'\n",
    "optimizer = 'AdamW'     # 'AdamW' or 'Adam' or 'SGD'\n",
    "lr = 1e-3\n",
    "initialization_scale = 8.0\n",
    "download_directory = \"../data\"\n",
    "weight_decay = 0\n",
    "depth = 3              # the number of nn.Linear modules in the model\n",
    "width = 200\n",
    "activation = 'ReLU'     # 'ReLU' or 'Tanh' or 'Sigmoid' or 'GELU'\n",
    "\n",
    "log_freq = math.ceil(optimization_steps / 150)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float64\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fn = activation_dict[activation]\n",
    "\n",
    "def create_mlp(depth, width, activation, alpha=1.0):\n",
    "    \"\"\"Creates an MLP model with specified depth, width, activation, and output scaling.\"\"\"\n",
    "    layers = [nn.Flatten()]\n",
    "    for i in range(depth):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(28 * 28, width))\n",
    "            layers.append(activation_fn())\n",
    "        elif i == depth - 1:\n",
    "            layers.append(nn.Linear(width, 10))\n",
    "        else:\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(activation_fn())\n",
    "\n",
    "    class OutputScaledMLP(nn.Module):\n",
    "        def __init__(self, mlp, alpha):\n",
    "            super().__init__()\n",
    "            self.mlp = mlp\n",
    "            self.alpha = alpha\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.alpha * self.mlp(x)\n",
    "\n",
    "    return OutputScaledMLP(nn.Sequential(*layers), alpha).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "def compute_accuracy(network, dataset, device, N=2000, batch_size=50):\n",
    "    \"\"\"Computes accuracy of `network` on `dataset`.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        N = min(len(dataset), N)\n",
    "        batch_size = min(batch_size, N)\n",
    "        dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, labels in islice(dataset_loader, N // batch_size):\n",
    "            logits = network(x.to(device))\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            correct += torch.sum(predicted_labels == labels.to(device))\n",
    "            total += x.size(0)\n",
    "        return (correct / total).item()\n",
    "\n",
    "def compute_loss(network, dataset, loss_function, device, N=2000, batch_size=50):\n",
    "    \"\"\"Computes mean loss of `network` on `dataset`.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        N = min(len(dataset), N)\n",
    "        batch_size = min(batch_size, N)\n",
    "        dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        loss_fn = loss_function_dict[loss_function](reduction='sum')\n",
    "        one_hots = torch.eye(10, 10).to(device)\n",
    "        total = 0\n",
    "        points = 0\n",
    "        for x, labels in islice(dataset_loader, N // batch_size):\n",
    "            y = network(x.to(device))\n",
    "            if loss_function == 'CrossEntropy':\n",
    "                total += loss_fn(y, labels.to(device)).item()\n",
    "            elif loss_function == 'MSE':\n",
    "                total += loss_fn(y, one_hots[labels]).item()\n",
    "            points += len(labels)\n",
    "        return total / points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train = torchvision.datasets.MNIST(root=download_directory, train=True,\n",
    "    transform=torchvision.transforms.ToTensor(), download=False)\n",
    "test = torchvision.datasets.MNIST(root=download_directory, train=False,\n",
    "    transform=torchvision.transforms.ToTensor(), download=False)\n",
    "train = torch.utils.data.Subset(train, range(train_points))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "subset_indices = np.random.choice(len(test), 960, replace=False) \n",
    "test_subset = torch.utils.data.Subset(test, subset_indices)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "assert activation in activation_dict, f\"Unsupported activation function: {activation}\"\n",
    "activation_fn = activation_dict[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ntk_at_epoch(model, dataloader):\n",
    "    # Set requires_grad to True for all model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    ntk_sum = None\n",
    "    num_batches = 0\n",
    "    \n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Compute the Jacobian matrix\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        jacobian = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            grad_output = torch.zeros_like(output)\n",
    "            grad_output[:] = 1.0\n",
    "            gradients = torch.autograd.grad(output, model.parameters(), grad_outputs=grad_output, create_graph=True)\n",
    "            jacobian.append(torch.cat([g.view(-1) for g in gradients]))\n",
    "        \n",
    "        jacobian = torch.stack(jacobian)\n",
    "        \n",
    "        # Compute the NTK for the current batch\n",
    "        ntk_batch = torch.matmul(jacobian, jacobian.t())\n",
    "        \n",
    "        # Accumulate the NTK sum\n",
    "        if ntk_sum is None:\n",
    "            ntk_sum = ntk_batch.cpu().detach()\n",
    "        else:\n",
    "            ntk_sum += ntk_batch.cpu().detach()\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    # Compute the average NTK over all batches\n",
    "    ntk_avg = ntk_sum / num_batches\n",
    "    \n",
    "    return ntk_avg.numpy()\n",
    "\n",
    "def kernel_distance(Kt1, Kt2):\n",
    "    \"\"\"\n",
    "    Compute the kernel distance between two Neural Tangent Kernels.\n",
    "    \n",
    "    Args:\n",
    "        Kt1: NTK matrix at time t1\n",
    "        Kt2: NTK matrix at time t2\n",
    "    \n",
    "    Returns:\n",
    "        The kernel distance between Kt1 and Kt2\n",
    "    \"\"\"\n",
    "    # Compute the Frobenius inner product\n",
    "    frobenius_inner_product = np.sum(Kt1 * Kt2)\n",
    "    \n",
    "    # Compute the Frobenius norms\n",
    "    frobenius_norm_Kt1 = np.sqrt(np.sum(Kt1**2))\n",
    "    frobenius_norm_Kt2 = np.sqrt(np.sum(Kt2**2))\n",
    "    \n",
    "    # Compute the kernel distance\n",
    "    kernel_dist = 1 - frobenius_inner_product / (frobenius_norm_Kt1 * frobenius_norm_Kt2)\n",
    "    \n",
    "    return kernel_dist\n",
    "\n",
    "\n",
    "def compute_ntk_full_at_epoch(model, dataloader):\n",
    "    '''\n",
    "    Compute NTK for full dataset loaded.\n",
    "    '''\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    all_jacobians = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        jacobian = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            grad_output = torch.zeros_like(output)\n",
    "            grad_output[:] = 1.0\n",
    "            gradients = torch.autograd.grad(output, model.parameters(), grad_outputs=grad_output, create_graph=True)\n",
    "            jacobian.append(torch.cat([g.view(-1) for g in gradients]))\n",
    "        \n",
    "        # stack Jacobians for this batch\n",
    "        jacobian = torch.stack(jacobian)\n",
    "        \n",
    "        # stack across batch\n",
    "        all_jacobians.append(jacobian)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # concatenate Jacobians and labels from all batches\n",
    "    full_jacobian = torch.cat(all_jacobians, dim=0)\n",
    "    full_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    # compute the NTK:\n",
    "    ntk = torch.mm(full_jacobian, full_jacobian.t())\n",
    "    \n",
    "    # return NTK and labels as NumPy arrays \n",
    "    return ntk.cpu().detach().numpy(), full_labels.cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_pairwise_distances(models, dataloader):\n",
    "    n = len(models)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            ntk_i, _ = compute_ntk_full_at_epoch(models[i], dataloader)\n",
    "            ntk_j, _ = compute_ntk_full_at_epoch(models[j], dataloader)\n",
    "            distance = kernel_distance(ntk_i, ntk_j)\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def plot_heatmap(distance_matrix, param_list, save_path):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(distance_matrix, annot=False, cmap='viridis')\n",
    "    plt.title(f'Kernel Distance Heatmap for {param_list}')\n",
    "    plt.xlabel('Model Checkpoint')\n",
    "    plt.ylabel('Model Checkpoint')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.5, 0.001]\n",
    "alpha = alphas[0]\n",
    "\n",
    "steps = [1, 10, 100, 500, 1000, 5000, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(alpha)\n",
    "    MLPmodels = []\n",
    "    mlp0 = create_mlp(depth, width, activation_fn, .5)\n",
    "    with torch.no_grad():\n",
    "        for p in mlp0.parameters():\n",
    "            p.data = initialization_scale * p.data\n",
    "\n",
    "    MLPmodels.append(mlp0)\n",
    "    for step in steps:\n",
    "        checkpoint = torch.load(os.path.join('/data/cici/Geometry/new/MNIST/checkpoints', f'alpha_{alpha}_{step}.pth'))\n",
    "        mlp = create_mlp(depth, width, activation_fn, .5)\n",
    "        mlp.load_state_dict(checkpoint)\n",
    "        MLPmodels.append(mlp)\n",
    "\n",
    "    distance_matrix = compute_pairwise_distances(MLPmodels, test_loader)\n",
    "    plot_heatmap(distance_matrix, f'MNIST, alpha = {alpha}',f'./K-dist_{alpha}_test.pdf')\n",
    "    np.save(f'K-dist_{alpha}_test.npy', distance_matrix)\n",
    "    distance_matrix = compute_pairwise_distances(MLPmodels, train_loader)\n",
    "    plot_heatmap(distance_matrix, f'MNIST, alpha = {alpha}',f'./K-dist_{alpha}_train.pdf')\n",
    "    np.save(f'K-dist_{alpha}_train.npy', distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
